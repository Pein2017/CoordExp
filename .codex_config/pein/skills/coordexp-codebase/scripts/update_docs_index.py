#!/usr/bin/env python3
"""
Update autogenerated navigation sections inside the `coordexp-codebase` skill.

Principles:
- Use local `docs/` as the source of truth.
- Index doc paths + their top-level H1 title (best-effort).
- Index code-navigation anchors from `src/`, `scripts/`, and `configs/`.
- Update only marker blocks (idempotent).
- Resolve runtime skill path from `CODEX_HOME` when available.
- Stdlib-only, so it runs in minimal environments.
"""

from __future__ import annotations

import argparse
import os
import re
import sys
from dataclasses import dataclass
from pathlib import Path
from typing import Iterable


AUTO_DOCS_START = "<!-- AUTO_DOCS_INDEX:START -->"
AUTO_DOCS_END = "<!-- AUTO_DOCS_INDEX:END -->"
AUTO_CODE_START = "<!-- AUTO_CODEBASE_INDEX:START -->"
AUTO_CODE_END = "<!-- AUTO_CODEBASE_INDEX:END -->"


@dataclass(frozen=True)
class DocItem:
    rel_path: str
    title: str
    category: str


@dataclass(frozen=True)
class CodeItem:
    rel_path: str
    label: str
    category: str


def _find_repo_root(start: Path) -> Path:
    start = start.resolve()
    for p in [start, *start.parents]:
        if (p / ".git").exists():
            return p
    raise RuntimeError(f"Could not find repo root from: {start}")



def _read_h1_title(md_path: Path) -> str:
    # Read a small prefix; title is expected near the top.
    try:
        with md_path.open("r", encoding="utf-8", errors="replace") as f:
            for _ in range(120):
                line = f.readline()
                if not line:
                    break
                line = line.strip()
                if line.startswith("# "):
                    return line[2:].strip()
    except OSError:
        pass
    return md_path.stem


def _sanitize_title(title: str, fallback: str) -> str:
    """
    Keep skill files ASCII-only; map common Unicode punctuation to ASCII.
    If the result still contains non-ASCII, fall back to a safe filename stem.
    """

    replacements = {
        "\u2192": "->",  # right arrow
        "\u00a0": " ",  # no-break space
        "\u2010": "-",  # hyphen
        "\u2013": "-",  # en dash
        "\u2014": "--",  # em dash
        "\u2018": "'",  # left single quote
        "\u2019": "'",  # right single quote
        "\u201c": '"',  # left double quote
        "\u201d": '"',  # right double quote
    }
    out = title
    for k, v in replacements.items():
        out = out.replace(k, v)
    return out if out.isascii() else fallback


def _categorize_doc(rel_path: Path) -> str:
    parts = rel_path.parts
    name_u = rel_path.name.upper()

    if len(parts) >= 2 and parts[1] == "data":
        return "Data + Datasets"

    if len(parts) >= 2 and parts[1] == "training":
        return "Training + Repro"

    if len(parts) >= 2 and parts[1] == "eval":
        return "Eval + Inference"

    if "ARCHITECTURE" in name_u:
        return "Training + Repro"

    if "STAGE2" in name_u or "ROLLOUT" in name_u:
        return "Stage-2 / Rollout Matching"

    if "INFER" in name_u or "EVALUATOR" in name_u or "DETECTION" in name_u:
        return "Eval + Inference"

    if (
        "PACKING" in name_u
        or "TRAINING" in name_u
        or "UPSTREAM" in name_u
        or "PORTING" in name_u
        or "COORD" in name_u
    ):
        return "Training + Repro"

    if "DATA" in name_u or "DATASET" in name_u or "PUBLIC" in name_u:
        return "Data + Datasets"

    return "Other"


def _iter_docs(repo_root: Path, docs_root: Path) -> list[DocItem]:
    if not docs_root.exists():
        return []

    md_files = sorted(p for p in docs_root.rglob("*.md") if p.is_file())
    items: list[DocItem] = []
    for p in md_files:
        rel = p.relative_to(repo_root).as_posix()
        raw_title = _read_h1_title(p)
        title = _sanitize_title(raw_title, fallback=p.stem)
        cat = _categorize_doc(Path(rel))
        items.append(DocItem(rel_path=rel, title=title, category=cat))
    return items


def _render_docs_index(items: Iterable[DocItem], update_cmd: str) -> str:
    by_cat: dict[str, list[DocItem]] = {}
    for it in items:
        by_cat.setdefault(it.category, []).append(it)

    cat_order = [
        "Data + Datasets",
        "Training + Repro",
        "Eval + Inference",
        "Stage-2 / Rollout Matching",
        "Other",
    ]

    out_lines: list[str] = []
    out_lines.append(
        f"_Autogenerated from `docs/**/*.md` (paths + H1 titles only). Run `{update_cmd}` to refresh._"
    )
    out_lines.append("")

    for cat in cat_order:
        cat_items = by_cat.get(cat, [])
        if not cat_items:
            continue
        out_lines.append(f"### {cat}")
        out_lines.append("")
        for it in sorted(cat_items, key=lambda x: x.rel_path):
            out_lines.append(f"- {it.title}: `{it.rel_path}`")
        out_lines.append("")

    return "\n".join(out_lines).rstrip() + "\n"


def _iter_code_items(repo_root: Path) -> list[CodeItem]:
    items: list[CodeItem] = []

    def add_path(rel_path: str, label: str, category: str) -> None:
        rel_clean = rel_path.rstrip("/")
        abs_path = repo_root / rel_clean
        if not abs_path.exists():
            return
        rendered_rel = f"{rel_clean}/" if abs_path.is_dir() else rel_clean
        items.append(CodeItem(rel_path=rendered_rel, label=label, category=category))

    core_entries = [
        ("src/sft.py", "Training entrypoint"),
        ("src/config/loader.py", "Config merge + prompt resolution"),
        ("src/config/prompts.py", "Prompt templates"),
        ("src/datasets/geometry.py", "Geometry helpers"),
        ("src/infer/pipeline.py", "Inference pipeline"),
        ("src/eval/detection.py", "Detection evaluator"),
    ]
    for rel_path, label in core_entries:
        add_path(rel_path, label, "Core Pipeline")

    trainers_dir = repo_root / "src/trainers"
    if trainers_dir.exists():
        for p in sorted(trainers_dir.glob("*.py")):
            if p.name == "__init__.py":
                continue
            rel = p.relative_to(repo_root).as_posix()
            items.append(CodeItem(rel_path=rel, label="", category="Trainer Modules"))

    add_path("src/datasets/augmentation/ops.py", "Augmentation ops", "Dataset Modules")
    datasets_dir = repo_root / "src/datasets"
    if datasets_dir.exists():
        for p in sorted(datasets_dir.glob("*.py")):
            if p.name in {"__init__.py", "geometry.py"}:
                continue
            rel = p.relative_to(repo_root).as_posix()
            items.append(CodeItem(rel_path=rel, label="", category="Dataset Modules"))

    scripts_root = repo_root / "scripts"
    script_keywords = ("train", "infer", "eval", "vis", "rollout", "stage2")
    if scripts_root.exists():
        for p in sorted(scripts_root.rglob("*")):
            if not p.is_file():
                continue
            if "__pycache__" in p.parts:
                continue
            if p.suffix.lower() not in {".py", ".sh"}:
                continue
            rel = p.relative_to(repo_root).as_posix()
            rel_l = rel.lower()
            if any(k in rel_l for k in script_keywords):
                items.append(CodeItem(rel_path=rel, label="", category="Runtime Scripts"))

    configs_root = repo_root / "configs"
    if configs_root.exists():
        for child in sorted(configs_root.iterdir(), key=lambda x: x.name):
            if child.name.startswith("."):
                continue
            if child.is_dir():
                count = sum(
                    1
                    for p in child.rglob("*")
                    if p.is_file() and p.suffix.lower() in {".yaml", ".yml"}
                )
                if count == 0:
                    continue
                rel = child.relative_to(repo_root).as_posix() + "/"
                label = "1 yaml file" if count == 1 else f"{count} yaml files"
                items.append(CodeItem(rel_path=rel, label=label, category="Config Groups"))
            elif child.is_file() and child.suffix.lower() in {".yaml", ".yml"}:
                rel = child.relative_to(repo_root).as_posix()
                items.append(
                    CodeItem(
                        rel_path=rel,
                        label="top-level config",
                        category="Config Groups",
                    )
                )

    src_root = repo_root / "src"
    if src_root.exists():
        for child in sorted(src_root.iterdir(), key=lambda x: x.name):
            if not child.is_dir() or child.name == "__pycache__":
                continue
            count = sum(
                1
                for p in child.rglob("*.py")
                if "__pycache__" not in p.parts
            )
            if count == 0:
                continue
            rel = child.relative_to(repo_root).as_posix() + "/"
            label = "1 python file" if count == 1 else f"{count} python files"
            items.append(
                CodeItem(rel_path=rel, label=label, category="Source Module Roots")
            )

    deduped: list[CodeItem] = []
    seen: set[tuple[str, str]] = set()
    for it in items:
        key = (it.category, it.rel_path)
        if key in seen:
            continue
        seen.add(key)
        deduped.append(it)
    return deduped


def _render_code_index(items: Iterable[CodeItem], update_cmd: str) -> str:
    by_cat: dict[str, list[CodeItem]] = {}
    for it in items:
        by_cat.setdefault(it.category, []).append(it)

    cat_order = [
        "Core Pipeline",
        "Trainer Modules",
        "Dataset Modules",
        "Runtime Scripts",
        "Config Groups",
        "Source Module Roots",
        "Other",
    ]

    out_lines: list[str] = []
    out_lines.append(
        f"_Autogenerated from repo path scan (`src/`, `scripts/`, `configs/`). Run `{update_cmd}` to refresh._"
    )
    out_lines.append("")

    for cat in cat_order:
        cat_items = by_cat.get(cat, [])
        if not cat_items:
            continue
        out_lines.append(f"### {cat}")
        out_lines.append("")
        for it in sorted(cat_items, key=lambda x: x.rel_path):
            if it.label:
                out_lines.append(f"- {it.label}: `{it.rel_path}`")
            else:
                out_lines.append(f"- `{it.rel_path}`")
        out_lines.append("")

    return "\n".join(out_lines).rstrip() + "\n"


def _replace_autogen_block(
    skill_text: str,
    new_block: str,
    start_marker: str,
    end_marker: str,
) -> str:
    if start_marker not in skill_text or end_marker not in skill_text:
        raise RuntimeError(
            f"Skill file missing required markers: {start_marker} / {end_marker}"
        )

    pattern = re.compile(
        re.escape(start_marker) + r".*?" + re.escape(end_marker),
        flags=re.DOTALL,
    )
    replacement = f"{start_marker}\n{new_block}{end_marker}"
    return pattern.sub(replacement, skill_text, count=1)


def _resolve_skill_from_codex_home() -> Path | None:
    codex_home = os.environ.get("CODEX_HOME", "").strip()
    if not codex_home:
        return None

    base = Path(codex_home).expanduser()
    candidates = [
        base / "skills" / "coordexp-codebase" / "SKILL.md",
        base / "coordexp-codebase" / "SKILL.md",
    ]
    for c in candidates:
        if c.exists():
            return c.resolve()
    return None


def _resolve_primary_skill_path(args: argparse.Namespace, script_path: Path, repo_root: Path) -> Path:
    if args.skill_path:
        candidate = (repo_root / args.skill_path).resolve()
        if not candidate.exists():
            raise RuntimeError(f"Primary skill file not found: {candidate}")
        return candidate

    codex_home_skill = _resolve_skill_from_codex_home()
    if codex_home_skill is not None:
        return codex_home_skill

    default_skill_path = (script_path.parents[1] / "SKILL.md").resolve()
    if not default_skill_path.exists():
        raise RuntimeError(f"Primary skill file not found: {default_skill_path}")
    return default_skill_path


def _default_update_cmd(script_path: Path, repo_root: Path) -> str:
    _ = script_path
    _ = repo_root
    return 'python3 "$CODEX_HOME/skills/coordexp-codebase/scripts/update_docs_index.py"'


def main(argv: list[str]) -> int:
    parser = argparse.ArgumentParser(description=__doc__)
    parser.add_argument(
        "--docs-root",
        default="docs",
        help="Docs directory (relative to repo root). Default: docs",
    )
    parser.add_argument(
        "--skill-path",
        default=None,
        help="Primary skill path (relative to repo root). Default: resolve from CODEX_HOME, then script-neighbor SKILL.md.",
    )
    parser.add_argument(
        "--check",
        action="store_true",
        help="Exit with code 2 if target skill file would change (do not write).",
    )
    args = parser.parse_args(argv)

    script_path = Path(__file__).resolve()
    repo_root = _find_repo_root(script_path)
    docs_root = repo_root / args.docs_root

    primary_skill_path = _resolve_primary_skill_path(args, script_path, repo_root)
    update_cmd = _default_update_cmd(script_path, repo_root)

    docs_items = _iter_docs(repo_root=repo_root, docs_root=docs_root)
    docs_block = _render_docs_index(docs_items, update_cmd=update_cmd)

    code_items = _iter_code_items(repo_root=repo_root)
    code_block = _render_code_index(code_items, update_cmd=update_cmd)

    old_text = primary_skill_path.read_text(encoding="utf-8", errors="replace")
    new_text = _replace_autogen_block(
        old_text,
        docs_block,
        AUTO_DOCS_START,
        AUTO_DOCS_END,
    )
    new_text = _replace_autogen_block(
        new_text,
        code_block,
        AUTO_CODE_START,
        AUTO_CODE_END,
    )

    if new_text == old_text:
        return 0

    if args.check:
        return 2

    primary_skill_path.write_text(new_text, encoding="utf-8")
    return 0


if __name__ == "__main__":
    raise SystemExit(main(sys.argv[1:]))
