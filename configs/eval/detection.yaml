# Offline detection evaluator configuration (CoordExp)
#
# Unified pipeline contract: evaluation consumes the inference artifact JSONL
# that embeds both GT and predictions per sample.

# For COCO/mAP, this should point to a scored artifact (gt_vs_pred_scored.jsonl)
# produced by `scripts/postop_confidence.py`.
# For f1ish-only evaluation, base gt_vs_pred.jsonl is allowed.
pred_jsonl: path/to/gt_vs_pred_scored.jsonl
out_dir: eval_out

# Metrics
metrics: f1ish                 # coco | f1ish | both
# Fixed-score toggle is intentionally removed: do not add `use_pred_score`.
use_segm: true                 # include segmentation metrics when polygons exist
strict_parse: false            # warn+skip malformed rows (bounded diagnostics)

# Optional COCO IoU thresholds override (null -> COCO defaults)
iou_thrs: null

# Diagnostics / overlays
overlay: false
overlay_k: 12
num_workers: 0

# Description matching always uses sentence-transformers/all-MiniLM-L6-v2; unmatched predictions are dropped (no bucket/drop fallback).
semantic_model: sentence-transformers/all-MiniLM-L6-v2
semantic_threshold: 0.6
semantic_device: auto
semantic_batch_size: 32

# F1-ish settings
f1ish_iou_thrs: [0.3, 0.5]
f1ish_pred_scope: annotated
