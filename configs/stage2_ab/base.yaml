# Stage-2 AB canonical profile base (standalone).
# This file intentionally does not extend configs/base.yaml.

global_max_length: 12000

model:
  torch_dtype: bfloat16
  attn_impl: flash_attention_2
  model: output/1-26/checkpoint-1516-merged

template:
  template: qwen3_vl
  truncation_strategy: raise
  # CoordExp forbids runtime image resizing; keep max_pixels fixed.
  max_pixels: 786432

tuner:
  train_type: lora
  use_swift_lora: false
  use_dora: true
  freeze_llm: false
  freeze_vit: false
  freeze_aligner: false
  target_modules: [all-linear]
  target_regex: null
  modules_to_save: []
  lora_rank: 16
  lora_alpha: 32
  lora_dropout: 0.0
  lora_bias: none

deepspeed:
  enabled: false

training:
  gradient_checkpointing: true
  lr_scheduler_type: cosine_warmup_with_min_lr
  bf16: true
  report_to: [tensorboard]
  seed: 17
  warmup_ratio: 0.05
  max_grad_norm: 1.0
  add_version: true
  save_safetensors: true
  save_total_limit: 2
  save_last_epoch: true
  save_only_model: true
  logging_steps: 10
  logging_first_step: true
  lr_scheduler_kwargs:
    min_lr: 1.0e-6
  num_train_epochs: 4
  per_device_train_batch_size: 1
  per_device_eval_batch_size: 4
  effective_batch_size: 64
  dataloader_drop_last: true
  packing: true
  packing_buffer: 256
  packing_min_fill_ratio: 0.5
  packing_drop_last: true
  eval_packing: false
  metric_for_best_model: rollout/f1
  greater_is_better: true
  optimizer: multimodal
  learning_rate: 2.0e-5
  vit_lr: 2.0e-5
  aligner_lr: 5.0e-5
  weight_decay: 0.0
  eval_strategy: steps
  eval_steps: 80
  save_strategy: steps
  save_steps: 200

data:
  # Required by ms-swift TrainArguments validation.
  dataset: [dummy]
  val_dataset: [dummy]
  dataset_num_proc: 8
  dataloader_num_workers: 16
  dataloader_pin_memory: true
  dataloader_prefetch_factor: 4
  dataloader_persistent_workers: true

stage2_ab:
  schedule:
    b_ratio: 0.05
  n_softctx_iter: 2
  bbox_smoothl1_weight: 2.0
  channel_b: {}

custom:
  json_format: standard
  emit_norm: none
  trainer_variant: stage2_ab_training
  train_jsonl: public_data/coco/rescale_32_768_bbox_max60/train.coord.jsonl
  val_jsonl: public_data/coco/rescale_32_768_bbox_max60/val.coord.jsonl
  train_sample_limit: null
  val_sample_limit: null
  coord_tokens:
    enabled: true
    skip_bbox_norm: true
  coord_soft_ce_w1:
    enabled: true

debug:
  enabled: false

rollout_matching:
  offload:
    enabled: false
    offload_model: false
    offload_optimizer: false
  rollout_backend: vllm
  decode_batch_size: 4
  vllm:
    mode: server
    tensor_parallel_size: 1
    gpu_memory_utilization: 0.85
    max_model_len: 12000
    enable_lora: false
    enable_prefix_caching: true
    sleep_level: 0
    server:
      timeout_s: 240.0
      infer_timeout_s: null
      servers:
        - base_url: http://127.0.0.1:8000
          group_port: 51216
    sync:
      mode: full
      fallback_to_full: true
  decode_mode: greedy
  max_new_tokens: 3084
  decoding:
    temperature: 0.0
    top_p: 1.0
    top_k: -1
  repetition_penalty: 1.08
  candidate_top_k: 5
  maskiou_gate: 0.3
  maskiou_resolution: 256
  fp_cost: 0.5
  fn_cost: 2
  ot_cost: l2
  ot_epsilon: 10
  ot_iters: 1
