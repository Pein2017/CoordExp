extends: ../base.yaml

model:
  model: output/1-26/checkpoint-1516-merged

tuner:
  train_type: lora
  use_swift_lora: false
  use_dora: true
  freeze_llm: false
  freeze_vit: false
  freeze_aligner: false
  target_modules: [all-linear]
  target_regex: null
  modules_to_save: []
  lora_rank: 16
  lora_alpha: 32
  lora_dropout: 0.0
  lora_bias: none

deepspeed:
  enabled: false

template:
  truncation_strategy: raise

training:
  num_train_epochs: 2
  per_device_train_batch_size: 1
  per_device_eval_batch_size: 4
  effective_batch_size: 64
  dataloader_drop_last: true
  packing: true
  packing_buffer: 256
  packing_min_fill_ratio: 0.5
  packing_drop_last: true
  eval_packing: false
  metric_for_best_model: rollout/f1
  greater_is_better: true
  optimizer: multimodal
  learning_rate: 2.0e-5
  vit_lr: 2.0e-5
  aligner_lr: 5.0e-5
  weight_decay: 0.0
  eval_strategy: steps
  eval_steps: 80
  save_strategy: steps
  save_steps: 200

stage2_ab:
  schedule:
    b_ratio: 0.05
  n_softctx_iter: 2
  bbox_smoothl1_weight: 2.0
  channel_b: {}

custom:
  trainer_variant: stage2_ab_training
  train_jsonl: public_data/lvis/rescale_32_768_bbox_max60/train.bbox_only.max60.coord.jsonl
  val_jsonl: public_data/lvis/rescale_32_768_bbox_max60/val.bbox_only.max60.coord.jsonl
  train_sample_limit: null
  val_sample_limit: null
  coord_tokens:
    enabled: true
    skip_bbox_norm: true
  coord_soft_ce_w1:
    enabled: true

rollout_matching:
  offload:
    enabled: false
    offload_model: false
    offload_optimizer: false
  rollout_backend: vllm
  decode_batch_size: 4
  vllm:
    mode: server
    tensor_parallel_size: 1
    gpu_memory_utilization: 0.85
    max_model_len: 12000
    enable_lora: false
    enable_prefix_caching: true
    sleep_level: 0
    server:
      timeout_s: 240.0
      infer_timeout_s: null
      servers:
        - base_url: http://127.0.0.1:8000
          group_port: 51216
    sync:
      mode: full
      fallback_to_full: true
  decode_mode: greedy
  max_new_tokens: 3084
  decoding:
    temperature: 0.0
    top_p: 1.0
    top_k: -1
  repetition_penalty: 1.08
  repeat_terminate:
    enabled: true
    min_new_tokens: 24
    max_consecutive_token_repeats: 24
    ngram_size: 24
    ngram_repeats: 2
    max_object_keys: 96
  candidate_top_k: 5
  maskiou_gate: 0.3
  maskiou_resolution: 256
  fp_cost: 0.5
  fn_cost: 2
  ot_cost: l2
  ot_epsilon: 10
  ot_iters: 1
