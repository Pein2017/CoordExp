# Smoke: stage2-ab B-only on LVIS bbox-only max60 (coord-token) starting from stage-1 merged ckpt-1516.
#
# Validates: vLLM rollout backend (greedy) -> parse/match -> teacher-forced loss (real backward).
extends: b_only.yaml

global_max_length: 8192

model:
  model: output/1-26/stage_1/poly_prefer_semantic_max60-pure_ce/v0-20260126-162638/epoch_4-pure_ce-LRs-2e-4_1e-4_4e-4-from-base-4B/merged_checkpoint-1516_20260128-130701

tuner:
  train_type: lora
  use_swift_lora: false
  use_dora: false
  freeze_llm: false
  freeze_vit: false
  freeze_aligner: false
  target_modules: [all-linear]
  target_regex: null
  modules_to_save: []
  lora_rank: 8
  lora_alpha: 16
  lora_dropout: 0.0
  lora_bias: none

deepspeed:
  enabled: false

training:
  output_dir: output/stage2_ab/smoke_bbox_max60_ckpt1516
  run_name: b_only_vllm_greedy_smoke
  logging_dir: .tb/stage2_ab/smoke_bbox_max60_ckpt1516/b_only_vllm_greedy_smoke

  max_steps: 1
  per_device_train_batch_size: 1
  per_device_eval_batch_size: 1
  gradient_accumulation_steps: 1

  logging_steps: 1
  logging_first_step: true

  eval_strategy: "no"
  save_strategy: "no"

data:
  dataloader_num_workers: 0
  dataloader_prefetch_factor: null
  dataloader_persistent_workers: false

custom:
  train_sample_limit: 2
  val_sample_limit: 1
  extra:
    rollout_matching:
      rollout_backend: vllm
      rollout_generate_batch_size: 1
      decode_mode: greedy
      max_new_tokens: 256
      temperature: 0.0
      repetition_penalty: 1.05
      vllm:
        mode: colocate
        tensor_parallel_size: 1
        # Keep this small so vLLM can coexist with the training model on a busy GPU.
        gpu_memory_utilization: 0.15
        max_model_len: 4096
        max_num_seqs: 1
        enable_lora: false
        enable_prefix_caching: true
        enforce_eager: true
        skip_mm_profiling: true
        limit_mm_per_prompt: {image: 1, video: 0}
        sleep_level: 0
