# Smoke: stage2-ab B-only on LVIS bbox-only max60 (coord-token) from stage-1 merged ckpt-1516,
# using vLLM **server mode** (rollouts on a separate GPU via `swift rollout`).
#
# Topology (single node, 2 GPUs):
# - GPU0: rollout server (swift rollout --infer_backend vllm)
# - GPU1: learner (python -m src.sft ...), world_size==1 (no torchrun)
#
# NOTE: vLLM server backend currently supports decode_mode=greedy only.
extends: b_only.yaml

global_max_length: 8192

model:
  model: output/1-26/stage_1/poly_prefer_semantic_max60-pure_ce/v0-20260126-162638/epoch_4-pure_ce-LRs-2e-4_1e-4_4e-4-from-base-4B/merged_checkpoint-1516_20260128-130701

tuner:
  # Keep this light for smoke runs.
  train_type: lora
  use_swift_lora: false
  use_dora: false
  freeze_llm: false
  freeze_vit: false
  freeze_aligner: false
  target_modules: [all-linear]
  target_regex: null
  modules_to_save: []
  lora_rank: 8
  lora_alpha: 16
  lora_dropout: 0.0
  lora_bias: none

deepspeed:
  enabled: false

training:
  output_dir: output/stage2_ab/smoke_bbox_max60_ckpt1516
  run_name: b_only_vllm_server_1v1_smoke
  logging_dir: .tb/stage2_ab/smoke_bbox_max60_ckpt1516/b_only_vllm_server_1v1_smoke

  max_steps: 1
  per_device_train_batch_size: 1
  per_device_eval_batch_size: 1
  gradient_accumulation_steps: 1

  logging_steps: 1
  logging_first_step: true

  eval_strategy: "no"
  save_strategy: "no"

data:
  dataloader_num_workers: 0
  dataloader_prefetch_factor: null
  dataloader_persistent_workers: false

custom:
  train_sample_limit: 2
  val_sample_limit: 1
  extra:
    rollout_matching:
      rollout_backend: vllm
      rollout_generate_batch_size: 1
      decode_mode: greedy
      max_new_tokens: 256
      temperature: 0.0
      repetition_penalty: 1.05

      vllm:
        mode: server
        # Full merged-weight sync is the robust default for multimodal models.
        enable_lora: false
        sync:
          mode: full
          fallback_to_full: true
        server:
          timeout_s: 240.0
          infer_timeout_s: null
          servers:
            - base_url: http://127.0.0.1:8000
              group_port: 51216

