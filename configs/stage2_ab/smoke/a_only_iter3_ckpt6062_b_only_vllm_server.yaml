# Production-like smoke: Stage-2 AB Channel-B only (rollout matching),
# using vLLM server mode and the provided checkpoint.
#
# This smoke config is intentionally close to prod behavior, but overrides:
# - model path (checkpoint under output/)
# - output dirs / run name
# - runtime knobs (max_steps + async queue sizing) to ensure B is exercised quickly.
extends:
  - ../prod/b_only.yaml
  - common_prodlike.yaml

model:
  model: output/stage2_ab/a_only_iter_3_ckpt_6062

training:
  output_dir: output/stage2_ab/smoke_a_only_iter3_ckpt6062/b_only_vllm_server
  logging_dir: .tb/stage2_ab/smoke_a_only_iter3_ckpt6062/b_only_vllm_server
  run_name: smoke_a_only_iter3_ckpt6062_b_only_vllm_server
  # Keep bounded: enough steps to exercise sync + rollouts + packing + backward.
  # Async prefetch may take some warmup time; keep enough steps to ensure at least one
  # Channel-B optimizer step executes (queue-feasibility gate).
  max_steps: 20
  # Smoke override: reduce accumulation so Channel-B feasibility gate can pass quickly.
  effective_batch_size: 6

stage2_ab:
  channel_b:
    async:
      # For an 8-GPU node with 4 learner ranks:
      # effective_batch_size=32 -> gradient_accumulation_steps=8, so queues must support >= 8 packs.
      queue_limit: 32
      prefetch_target_packs: 16
      version_window: 100
      sync_every_steps: 2

custom:
  extra:
    rollout_matching:
      # Smoke override: keep rollouts short so async queues can fill quickly.
      # NOTE: This is a speed-oriented smoke setting; production uses much longer rollouts.
      max_new_tokens: 64
      vllm:
        server:
          # Smoke override: fail fast if server inference stalls so async prefetch
          # surfaces actionable errors instead of silently blocking for minutes.
          infer_timeout_s: 30.0
