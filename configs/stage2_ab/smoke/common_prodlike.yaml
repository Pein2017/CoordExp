# Common smoke overrides intended to keep hyperparameters identical to production,
# while reducing runtime via small sample limits and minimal I/O.
#
# IMPORTANT:
# - Do not override core training/rollout hyperparameters here (LRs, max_new_tokens, etc.).
# - Only override runtime knobs: output dirs, run_name, sample limits, max_steps, dataloader.
training:
  # Keep smoke bounded while still exercising the trainer end-to-end.
  # NOTE: For dense AB schedules (e.g. b_ratioâ‰ˆ1/3), 3 steps is the minimum to hit a B step.
  # For sparse Channel-B (e.g. b_ratio=0.05), you may need ~20 steps to hit one B step.
  max_steps: 3

  # Avoid disk churn for smoke/regression runs.
  save_strategy: "no"
  # Keep smoke runs lightweight (no eval loop).
  eval_strategy: "no"
  # Log every step so smoke has a visible loss trend.
  logging_steps: 1

data:
  dataloader_num_workers: 0
  dataloader_prefetch_factor: null
  dataloader_persistent_workers: false

custom:
  # Small sample caps: enough to cover a few optimizer steps with effective_batch_size=32.
  train_sample_limit: 64
  val_sample_limit: 8
