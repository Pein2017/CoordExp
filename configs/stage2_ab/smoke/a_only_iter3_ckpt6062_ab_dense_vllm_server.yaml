# Production-like smoke: Stage-2 AB mixed schedule with a dense B ratio
# so that both A and B paths execute within a few steps, using vLLM server mode
# and the provided checkpoint.
extends:
  - ../prod/base.yaml
  - common_prodlike.yaml

model:
  model: output/stage2_ab/a_only_iter_3_ckpt_6062

training:
  output_dir: output/stage2_ab/smoke_a_only_iter3_ckpt6062/ab_dense_vllm_server
  logging_dir: .tb/stage2_ab/smoke_a_only_iter3_ckpt6062/ab_dense_vllm_server
  run_name: smoke_a_only_iter3_ckpt6062_ab_dense_vllm_server
  max_steps: 50
  # Smoke override: reduce accumulation so Channel-B feasibility gate can pass quickly.
  effective_batch_size: 32

stage2_ab:
  schedule:
    # Dense schedule so B is hit in smoke quickly.
    b_ratio: 0.5
  channel_b:
    async:
      # Target topology for this smoke: 6 rollout GPUs (server) + 2 learner GPUs (DDP).
      #
      # With effective_batch_size=32 and per_device_train_batch_size=1:
      # - world_size=2 => gradient_accumulation_steps (GAS) = 16
      #
      # Async feasibility gate requires >= GAS ready packs of a single eligible version
      # per rank, so prefetch/queue should be sized as a multiple of GAS.
      queue_limit: 64
      prefetch_target_packs: 32
      version_window: 8
      sync_every_steps: 4

custom:
  extra:
    rollout_matching:
      # Smoke override: keep rollouts short so async queues can fill quickly.
      # NOTE: This is a speed-oriented smoke setting; production uses much longer rollouts.
      max_new_tokens: 3084
      # Qualitative monitoring dumps (rank0 only): prompt + rollout + parsed objects + training target.
      # Writes to: <output_dir>/monitor_dumps/step_000000.{json,md}, ...
      monitor_dump:
        enabled: true
        # Dump every N optimizer steps (global_step). Also respects `max_events`.
        every_steps: 4
        max_events: 20
        max_samples: 1
        max_text_chars: 4000
      vllm:
        server:
          # Smoke override: fail fast if server inference stalls so async prefetch
          # surfaces actionable errors instead of silently blocking for minutes.
          infer_timeout_s: 300.0
          # Raw vLLM server rollout dump (token ids + response text + parse summary).
          # Writes to: <output_dir>/vllm_server_debug/step_000000_event_001.json, ...
          debug_dump:
            enabled: true
            # Dump once every N optimizer steps (global_step). Respects `max_events`.
            every_steps: 4
            dump_first_step: true
            # Avoid rank collisions / I/O storms under DDP.
            only_world_process_zero: true
            max_events: 20
            max_samples: 1
            max_chars: 4000
