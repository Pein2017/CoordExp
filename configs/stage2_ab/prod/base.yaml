# Production Stage-2 AB base: LVIS bbox-only max60 from ckpt-1516 (merged),
# vLLM server mode, DoRA ("dlora"), and num_train_epochs=2.
#
# Child variants should primarily override schedule and run/output naming.
extends: ../base_rollout_matching_sft.yaml

model:
  model: output/1-26/checkpoint-1516-merged

tuner:
  train_type: lora
  use_swift_lora: false
  use_dora: true
  freeze_llm: false
  freeze_vit: false
  freeze_aligner: false
  target_modules: [all-linear]
  target_regex: null
  modules_to_save: []
  lora_rank: 16
  lora_alpha: 32
  lora_dropout: 0.0
  lora_bias: none

deepspeed:
  enabled: false

training:
  # Variants should override output_dir/logging_dir to avoid clobbering checkpoints.
  output_dir: output/stage2_ab/prod_bbox_max60_ckpt1516_ablation_ep2
  logging_dir: tb/stage2_ab/prod_bbox_max60_ckpt1516_ablation_ep2

  num_train_epochs: 2
  # GPU memory constraint: one packed sequence per backward.
  per_device_train_batch_size: 1
  per_device_eval_batch_size: 1

  # Auto-computes gradient_accumulation_steps based on world_size:
  # effective_batch_size = per_device_train_batch_size * world_size * grad_accum
  # Keep high effective batch for optimizer stability (default launcher uses 2 learners).
  effective_batch_size: 64
  # Required for Channel-B step mode to avoid partial accumulation windows.
  dataloader_drop_last: true

  # Rollout evaluation is expensive; keep it sparse.
  eval_steps: 80

  save_strategy: "steps"
  save_steps: 200

  optimizer: multimodal
  learning_rate: 2.0e-5
  vit_lr: 2.0e-5
  aligner_lr: 5.0e-5
  weight_decay: 0.0

stage2_ab:
  # Default prod variant is mixed schedule; specific variants can override.
  schedule:
    b_ratio: 0.05
  # Non-default: stronger bbox regression pressure in prod.
  bbox_smoothl1_weight: 2.0
  channel_b:
    # Downweight desc CE for matched objects; keep missing-object learning strict.
    desc_ce_weight_matched: 0.2
    semantic_desc_gate:
      # Required for reproducibility when semantic gate is enabled.
      revision: c9745ed1d9f207416be6d2e6f8de32d1f16199bf
    # Simpler/reliable default: synchronous Channel-B execution (no async queue/gate).
    mode: micro

custom:
  trainer_variant: stage2_ab_training

  # Full dataset (no smoke sampling caps).
  train_sample_limit: null
  val_sample_limit: null

  extra:
    rollout_matching:
      max_new_tokens: 3084
      # Mild anti-loop bias for long dense JSON.
      repetition_penalty: 1.08
      # Per-rank rollout request chunk size (decode batch size per generation call).
      rollout_infer_batch_size: 4
      # Window scope improves post-rollout packing fill under long generations.
      post_rollout_pack_scope: window

      vllm:
        mode: server
        # Full merged-weight sync is the robust default for multimodal models.
        enable_lora: false
        sync:
          mode: full
          fallback_to_full: true
        server:
          timeout_s: 240.0
          # If null, the trainer defaults to timeout_s.
          infer_timeout_s: null
          servers:
            - base_url: http://127.0.0.1:8000
              group_port: 51216
