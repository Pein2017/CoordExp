# Shared Stage-2 rollout-matching defaults for Stage-2 A/B configs.
#
# This base stays runnable in self-contained colocate mode.
# Production server-mode behavior is enabled in `configs/stage2_ab/prod/base.yaml`.

extends: ../base.yaml

template:
  # Stage-2 SFT must never silently truncate; fail fast on any over-length sample.
  truncation_strategy: raise

training:
  # Stage_2 supports *post-rollout* packing for the teacher-forced forward pass.
  # Rollout generation remains un-packed (padded batch) inside the trainer.
  packing: true
  packing_buffer: 256
  packing_min_fill_ratio: 0.5
  packing_drop_last: true
  # eval_packing is ignored for rollout-matching (trainer handles its own collation).
  eval_packing: false

custom:
  # Opt-in trainer variant (stage_2).
  trainer_variant: rollout_matching_sft

  # Update these paths for your run.
  train_jsonl: public_data/lvis/rescale_32_768_bbox_max60/train.bbox_only.max60.coord.jsonl
  val_jsonl: public_data/lvis/rescale_32_768_bbox_max60/val.bbox_only.max60.coord.jsonl

  # Rollout-matching assumes coord-token mode for coord supervision.
  coord_tokens:
    enabled: true
    # Coord-token JSONLs are pre-normalized; avoid double-normalization.
    skip_bbox_norm: true
  coord_soft_ce_w1:
    enabled: true


  # Rollout/matching knobs live under custom.extra.rollout_matching.
  extra:
    rollout_matching:
      # Colocate vLLM offload (peak memory relief during rollouts only).
      # This wraps vLLM engine init + LoRA load/sync + infer in an offload context.
      #
      # Not supported with DeepSpeed/ZeRO in this trainer; if needed, disable offload or switch rollout_backend: hf.
      offload:
        enabled: false
        offload_model: false
        offload_optimizer: false

      # Rollout backend. Set to "hf" to bypass vLLM entirely.
      rollout_backend: vllm

      # Per-rank rollout request chunk size.
      # - rollout_generate_batch_size is the legacy knob.
      # - stage2_ab trainer prefers rollout_infer_batch_size when set.
      rollout_generate_batch_size: 1
      rollout_infer_batch_size: 1

      # vLLM knobs.
      # - mode=colocate: learner instantiates local vLLM engines on training GPUs.
      # - mode=server: learner connects to external `swift rollout` server(s) on actor GPUs.
      vllm:
        # Keep base self-contained by default; prod/server variants override this to `server`.
        mode: colocate  # colocate|server
        # Colocate-only knobs:
        tensor_parallel_size: 1
        gpu_memory_utilization: 0.75
        # Must cover prompt_len + max_new_tokens for stage_2 rollouts.
        max_model_len: 12000
        # Keep disabled by default. Enable only when using adapter-only server sync.
        enable_lora: false
        enable_prefix_caching: true
        sleep_level: 0
        # Server-mode connectivity + sync (ignored in colocate mode):
        server:
          timeout_s: 240.0
          infer_timeout_s: null
          servers:
            - base_url: http://127.0.0.1:8000
              group_port: 51216
        sync:
          mode: full        # full|adapter|auto
          fallback_to_full: true

      # Decoding (greedy only)
      decode_mode: greedy
      max_new_tokens: 512
      decoding:
        temperature: 0.0
        top_p: 1.0
        top_k: -1
      # Mild anti-loop bias (empirically helpful on Qwen3-VL for long dense JSON).
      repetition_penalty: 1.05

      # Repeat-aware safety guard: force EOS early for offending sequences when
      # generation becomes degenerate/repetitive (HF + vLLM server mode).
      repeat_terminate:
        enabled: true
        min_new_tokens: 32
        max_consecutive_token_repeats: 64
        ngram_size: 64
        ngram_repeats: 2
        # Optional hard cap to prevent runaway "object_k" spam.
        max_object_keys: 256

      # Matching
      candidate_top_k: 5
      maskiou_gate: 0.3
      maskiou_resolution: 256
      fp_cost: 0.5
      fn_cost: 2

      # Poly OT targets (Sinkhorn + barycentric projection)
      ot_cost: l2
      ot_epsilon: 10
      ot_iters: 1
