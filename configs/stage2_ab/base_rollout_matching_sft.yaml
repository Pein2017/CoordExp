# Rollout-matching SFT (stage_2) template.
#
# This is a PLACEHOLDER config intended for YAML-first iteration.
# Fill in paths/checkpoints and tune rollout/matching knobs as needed.

extends: ../base.yaml

template:
  # Stage-2 SFT must never silently truncate; fail fast on any over-length sample.
  truncation_strategy: raise

training:
  # Stage_2 supports *post-rollout* packing for the teacher-forced forward pass.
  # Rollout generation remains un-packed (padded batch) inside the trainer.
  packing: true
  packing_buffer: 256
  packing_min_fill_ratio: 0.5
  packing_drop_last: true
  # eval_packing is ignored for rollout-matching (trainer handles its own collation).
  eval_packing: false

custom:
  # Opt-in trainer variant (stage_2).
  trainer_variant: rollout_matching_sft

  # Update these paths for your run.
  train_jsonl: public_data/lvis/rescale_32_768_bbox_max60/train.bbox_only.max60.coord.jsonl
  val_jsonl: public_data/lvis/rescale_32_768_bbox_max60/val.bbox_only.max60.coord.jsonl

  # Rollout-matching assumes coord-token mode for coord supervision.
  coord_tokens:
    enabled: true
    # Coord-token JSONLs are pre-normalized; avoid double-normalization.
    skip_bbox_norm: true
  coord_soft_ce_w1:
    enabled: true


  # Rollout/matching knobs live under custom.extra.rollout_matching.
  extra:
    rollout_matching:
      # Colocate vLLM offload (peak memory relief during rollouts only).
      # This wraps vLLM engine init + LoRA load/sync + infer in an offload context.
      #
      # Not supported with DeepSpeed/ZeRO in this trainer; if needed, disable offload or switch rollout_backend: hf.
      offload:
        enabled: false
        offload_model: false
        offload_optimizer: false

      # Rollout backend (default: vLLM colocate). Set to "hf" to bypass vLLM.
      rollout_backend: vllm

      # Per-rank microbatch size for rollout generation (HF uses padded generate(); vLLM chunks requests).
      rollout_generate_batch_size: 1

      # vLLM knobs.
      # - mode: colocate (default): learner instantiates a local vLLM engine (uses VRAM on training GPUs).
      # - mode: server (optional): learner connects to a pre-launched `swift rollout` server and generates rollouts on
      #   dedicated GPUs with in-memory weight sync.
      # NOTE: tensor_parallel_size/gpu_memory_utilization/max_model_len are used only in colocate mode.
      vllm:
        mode: server  # colocate|server
        tensor_parallel_size: 4
        gpu_memory_utilization: 0.7
        # REQUIRED: must cover prompt_len + max_new_tokens for stage_2 rollouts.
        max_model_len: 12000
        # REQUIRED for on-policy LoRA sync.
        enable_lora: true
        enable_prefix_caching: true
        sleep_level: 0
        # Server mode connectivity + sync (ignored in colocate mode):
        # server:
        #   timeout_s: 240.0
        #   infer_timeout_s: null
        #   servers:
        #     - base_url: http://127.0.0.1:8000
        #       group_port: 51216
        # sync:
        #   mode: full        # full|adapter|auto
        #   fallback_to_full: true

      # Decoding (greedy only)
      decode_mode: greedy
      max_new_tokens: 512
      decoding:
        temperature: 0.0
        top_p: 1.0
        top_k: -1
      # Mild anti-loop bias (empirically helpful on Qwen3-VL for long dense JSON).
      repetition_penalty: 1.05

      # HF-only safety guard: force EOS early if generation becomes degenerate/repetitive.
      # (vLLM currently ignores this hook.)
      repeat_terminate:
        enabled: true
        min_new_tokens: 32
        max_consecutive_token_repeats: 64
        ngram_size: 64
        ngram_repeats: 2
        # Optional hard cap to prevent runaway "object_k" spam.
        max_object_keys: 256

      # Matching
      candidate_top_k: 5
      maskiou_gate: 0.3
      maskiou_resolution: 256
      fp_cost: 0.5
      fn_cost: 2

      # Poly OT targets (Sinkhorn + barycentric projection)
      ot_cost: l2
      ot_epsilon: 10
      ot_iters: 1
