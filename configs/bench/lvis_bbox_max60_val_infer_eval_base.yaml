# Base config for LVIS bbox-only validation inference + evaluation.
#
# This repo's unified inference pipeline treats YAML configs as single files:
# - no extends/inherit across files
# - no variable interpolation
#
# To create a new run, copy this file and edit:
# - run.name
# - infer.model_checkpoint
# - infer.limit (optional)
#
# Entrypoint:
#   PYTHONPATH=. conda run -n ms python scripts/run_infer.py --config <this.yaml>

run:
  name: TEMPLATE
  output_dir: output/bench

stages:
  infer: true
  eval: true
  vis: false

infer:
  gt_jsonl: public_data/lvis/rescale_32_768_bbox_max60/val.bbox_only.max60.coord.jsonl
  model_checkpoint: output/stage2_ab/REPLACE_ME

  # coord | text | auto
  mode: auto
  pred_coord_mode: auto

  backend:
    # hf | vllm
    type: hf

    # If you switch to vllm, you typically want an externally managed server:
    # type: hf
    base_url: http://127.0.0.1:8000
    auto_launch: false
    timeout_s: 180.0
    client_concurrency: 8

  generation:
    temperature: 0.1
    top_p: 0.9
    max_new_tokens: 3084
    repetition_penalty: 1.1
    # Batch decoding:
    # - HF: micro-batched `model.generate()` when > 1
    # - vLLM: concurrent requests per flush when > 1
    batch_size: 8
    seed: 42

  device: cuda:0
  limit: 0
  detect_samples: 128

eval:
  output_dir: null  # defaults to <run_dir>/eval
  metrics: both
  use_segm: false
  overlay: false
  overlay_k: 12
  num_workers: 8
