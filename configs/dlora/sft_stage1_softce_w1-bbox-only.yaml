extends: sft_base.yaml

# Scheme A (Stage-1 retrain): CE on non-coord tokens + coord-gated softCE+W1 on coord tokens.
# This config is intentionally placeholder-heavy; tune per run.

global_max_length: 16000

model:
  # Base checkpoint (expanded vocab with <|coord_*|> tokens)
  model: model_cache/models/Qwen/Qwen3-VL-4B-Instruct-coordexp

training:
  output_dir: ./output/1-24/stage_1/poly_prefer_semantic_max60
  run_name: epoch_4-softce_w1-LRs-2e-4_1e-4_4e-4-from-base-4B
  logging_dir: ./tb/1-24/stage_1/poly_prefer_semantic_max60
  num_train_epochs: 4
  per_device_train_batch_size: 1
  per_device_eval_batch_size: 1
  effective_batch_size: 32
  # Use the custom optimizer variant so coord_offset can have its own lr buckets.
  optimizer: multimodal_coord_offset
  learning_rate: 2.0e-4
  vit_lr: 1.0e-4
  aligner_lr: 4.0e-4
  packing: true
  packing_buffer: 512
  packing_min_fill_ratio: 0.7
  packing_drop_last: true
  eval_packing: true
  eval_steps: 40
  save_steps: 200
  save_total_limit: 2
  save_delay_steps: 800

custom:
  train_jsonl: public_data/lvis/rescale_32_768_bbox_max60/train.bbox_only.max60.coord.jsonl
  val_jsonl: public_data/lvis/rescale_32_768_bbox_max60/val.bbox_only.max60.coord.jsonl
  val_sample_limit: 1000
  val_sample_with_replacement: false
  token_type_metrics:
    enabled: true
  # Train only the 1000 newly added coord-token IDs via a lightweight offset adapter
  # (keeps the adapter small vs saving full embed/lm_head matrices).
  coord_offset:
    enabled: true
    ids: { start: 151670, end: 152669 }  # <|coord_0|>.. <|coord_999|>
    # Optional: override per-bucket lrs; defaults to training.learning_rate when omitted.
    embed_lr: 1.0e-4
    head_lr: 1.0e-4
    weight_decay: 0.0
  coord_soft_ce_w1:
    enabled: true
    # Loss composition at coord positions: soft_ce_weight * softCE + w1_weight * W1(CDF)
    soft_ce_weight: 1.0
    w1_weight: 3.0
    gate_weight: 5.0
    # Distribution temperature for coord-gated softmax.
    temperature: 0.9
    # Gaussian soft target over bins 0..999.
    target_sigma: 1.5
    target_truncate: 8

debug:
  enabled: false
  output_dir: ./output_debug/
  train_sample_limit: 256
  val_sample_limit: 64
