extends: sft_base.yaml

# Override base length for packing; merged vision tokens included in template max length.
global_max_length: 12000

training:
  # Override base training settings
  learning_rate: 2.0e-4
  vit_lr: 1.0e-4
  aligner_lr: 4.0e-4
  num_train_epochs: 4
  optimizer: multimodal  # No coord_offset optimizer needed for pure string data
  output_dir: ./output/12-4/text_only
  run_name: epoch_4-dlora-lrs_2_1_4-sorted-text_only-pre-normalized-packing-eff_bs_12-max_length_16000
  logging_dir: ./tb/12-4/text_only
  save_strategy: steps
  eval_steps: 100
  save_delay_steps: 2000
  # Packing yields one packed sample per device; effective_batch_size=12 with world_size=4
  # and per-device=1 targets ~117 base samples/update with avg ~9.7 samples per pack (post-merge).
  per_device_train_batch_size: 1
  per_device_eval_batch_size: 1
  effective_batch_size: 12
  # Packing knobs (see docs/data/PACKING.md for defaults/tuning)
  packing_buffer: 256
  packing_min_fill_ratio: 0.7
  packing_drop_last: true
  eval_packing: true


custom:
  train_jsonl: public_data/lvis/rescale_32_768_bbox_max60/train.bbox_only.max60.coord.jsonl
  val_jsonl: public_data/lvis/rescale_32_768_bbox_max60/val.bbox_only.max60.coord.jsonl
  # Pre-normalized JSONLs; no runtime scaling
  emit_norm: none
  # Coord-token-only contract in this repo.
  coord_tokens:
    enabled: true
    skip_bbox_norm: true
  # ABLATION: Disable coord_offset adapter (no need to tune lm_head/token_embedding for pure strings)
  coord_offset:
    enabled: false
    tie_head: true
