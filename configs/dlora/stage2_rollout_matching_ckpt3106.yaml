extends: ../base.yaml

# Stage_2 rollout-matching with buffered rollouts (production-style defaults) starting from ckpt-3106.
#
# This file is meant to be "ready for production" with buffered rollouts (E-step generate once, M-step reuse).
# Uses real dataset, long max length, vLLM rollouts configured with monitoring optimizations.
# For a quick smoke on a handful of real samples, use:
#   configs/dlora/smoke_stage2_rollout_matching_ckpt3106.yaml

# Keep length long (packing length is derived from this value).
global_max_length: 12000

tuner:
  # DoRA ("dlora"): restrict LoRA injection to:
  # - last 8 LLM blocks (layers 28-35 for Qwen3-VL-8B, 36 layers total)
  # - all MLP aligner modules (visual merger + deepstack mergers)
  train_type: lora
  use_dora: true
  freeze_llm: false
  freeze_vit: true
  freeze_aligner: false
  target_modules: [all-linear]
  # NOTE: PEFT uses `re.fullmatch` when `target_modules` is a regex string.
  # Keep this regex *precise* (Linear-only) to avoid matching LayerNorm/containers, which PEFT cannot wrap with LoRA.
  # Keep this as a SINGLE LINE (no YAML folding), otherwise whitespace becomes part of the regex and LoRA/DoRA can
  # silently match nothing.
  #
  # Matches:
  # - LLM layers 28-35: {q,k,v,o}_proj + {gate,up,down}_proj
  # - Aligner MLPs: visual.merger + deepstack_merger_list.{0,1,2} linear_fc{1,2}
  target_regex: '^(model\.visual\.merger\.(linear_fc1|linear_fc2)|model\.visual\.deepstack_merger_list\.(0|1|2)\.(linear_fc1|linear_fc2)|model\.language_model\.layers\.(2[8-9]|3[0-5])\.(self_attn\.(q_proj|k_proj|v_proj|o_proj)|mlp\.(gate_proj|up_proj|down_proj)))$'

model:
  # Use absolute path to avoid hub fallback if cwd differs across launchers/ranks.
  model: output/12-24/coord_loss-merged/ckpt-3106
  # SDPA is more robust for long-context monitoring runs.
  attn_impl: flash_attention_2

training:
  output_dir: output/rollout_matching_sft/stage2_ckpt3106_buffered
  run_name: debug
  logging_dir: .tb/stage2_ckpt3106_buffered

  num_train_epochs: 2
  # Rollout-matching is rollout-heavy; keep per-device small for stability on long sequences.
  per_device_train_batch_size: 1
  # Keep aligned with the single rollout batching knob:
  # rollout_matching.decode_batch_size.
  per_device_eval_batch_size: 4
  # On 4xA100 this yields grad_acc=16 (more stable than grad_acc=1 at long lengths).
  effective_batch_size: 32

  # Dense per-step logging for rollout metrics (recall, truncation, packing fill, etc.).
  logging_steps: 1
  logging_first_step: true

  # For buffered rollouts, prefer full windows only.
  dataloader_drop_last: true

  # Rollout-matching already logs core rollout metrics during training; disable eval by default.
  eval_strategy: steps
  eval_steps: 1
  # Save periodically (no eval dependency).
  save_strategy: steps
  save_steps: 80
  save_total_limit: 2
  save_delay_steps: 200

  # Rollout-matching eval runs full rollout->parse->Hungarian-match, so use a
  # detection-like metric as best-model monitor.
  metric_for_best_model: rollout/f1
  greater_is_better: true

  optimizer: multimodal
  learning_rate: 2.0e-5
  vit_lr: 2.0e-5
  aligner_lr: 5.0e-5
  weight_decay: 0.1

  # Stage_2 supports post-rollout packing inside the trainer only.
  packing: true
  packing_buffer: 512
  packing_min_fill_ratio: 0.7
  packing_drop_last: true
  # eval_packing is ignored for rollout-matching (trainer handles collation).
  eval_packing: true

custom:
  trainer_variant: stage2_rollout_aligned

  # Real LVIS coord-token dataset (update paths if needed).
  train_jsonl: public_data/lvis/rescale_32_768_bbox_max60/train.bbox_only.max60.coord.jsonl
  val_jsonl: public_data/lvis/rescale_32_768_bbox_max60/val.bbox_only.max60.coord.jsonl

  # Limit samples for monitoring runs.
  train_sample_limit: 128
  val_sample_limit: 32

rollout_matching:
  # Default: vLLM colocate rollouts (teacher-forced forward/backprop stays on the training model).
  # For monitoring/debugging, you can switch to HF backend: rollout_backend: hf
  rollout_backend: vllm
  # Unified rollout decode batch size (per device per generation call).
  decode_batch_size: 4
  pipeline:
    objective:
      - name: token_ce
        enabled: true
        weight: 1.0
        channels: [A, B]
        config:
          desc_ce_weight: 1.0
          self_context_struct_ce_weight: 0.0
          rollout_fn_desc_weight: 1.0
          rollout_matched_prefix_struct_weight: 1.0
          rollout_drop_invalid_struct_ce_multiplier: 1.0
      - name: bbox_geo
        enabled: true
        weight: 1.0
        channels: [A, B]
        config:
          smoothl1_weight: 1.0
          ciou_weight: 1.0
          a1_smoothl1_weight: 0.0
          a1_ciou_weight: 0.0
      - name: coord_reg
        enabled: true
        weight: 1.0
        channels: [A, B]
        config:
          coord_ce_weight: 0.0
          coord_el1_weight: 0.0
          coord_ehuber_weight: 0.0
          coord_huber_delta: 0.001
          coord_entropy_weight: 0.0
          soft_ce_weight: 1.0
          self_context_soft_ce_weight: 0.0
          w1_weight: 1.0
          a1_soft_ce_weight: 0.0
          a1_w1_weight: 0.0
          coord_gate_weight: 1.0
          text_gate_weight: 0.0
          temperature: 1.0
          target_sigma: 2.0
          target_truncate: 16
    diagnostics:
      - name: coord_diag
        enabled: true
        weight: 1.0
        channels: [A, B]
        config: {}

  # REQUIRED for vLLM rollouts (colocate). Must cover prompt_len + max_new_tokens.
  vllm:
    # Keep max_num_seqs close to decode_batch_size for lower memory overhead.
    # For monitoring runs, you can use: max_num_seqs: 4
    max_num_seqs: 4
    infer_batch_size: 1

    tensor_parallel_size: 1
    # Keep this modest to leave headroom for the training model on the same GPU.
    gpu_memory_utilization: 0.7
    # Chunk TP-group gathered requests to reduce peak multimodal prefill VRAM in colocate mode.
    # Must cover prompt_len + max_new_tokens (vLLM requirement).
    #
    # With max_model_len=16000 and max_new_tokens=8192, your prompt_len must be <= 7808,
    # otherwise rollout generations may truncate or error. Increase max_model_len if needed.
    max_model_len: 16000
    # vLLM+LoRA on multimodal models can be unstable under torch.compile graphs.
    # enforce_eager disables compilation (slower but much more robust).
    enforce_eager: true
    # vLLM does a huge multimodal (video) dummy profile run by default, which can trip LoRA kernels.
    # We only need image rollouts here; skip multimodal profiling for stability.
    skip_mm_profiling: true
    # NOTE: `mm_encoder_tp_mode: data` can reduce peak vision prefill memory,
    # but we have observed runtime shape issues with our current vLLM stack.
    # Keep the safe default (`weights`) for production readiness unless you
    # have verified `data` works on your machine/version.
    mm_encoder_tp_mode: weights
    # Defensive multimodal limits: we only use single-image samples in CoordExp.
    limit_mm_per_prompt: {image: 1, video: 0}
    # Chunk long prefills to reduce peak memory during the prefill phase.
    enable_chunked_prefill: true
    # Some vLLM versions can behave poorly when chunking multimodal (image) inputs.
    # Keep multimodal inputs unchunked while still chunking long text prefills.
    disable_chunked_mm_input: true
    # vLLM internal scheduling knobs. These do NOT change the model context length, but can
    # affect whether vLLM LoRA kernels have enough token budget during profile/run.

    # Must be >= the maximum token count seen by any LoRA-wrapped linear op
    # (can include vision tokens). Keeping this close to max_model_len
    # reduces vLLM internal preallocations in colocate mode.
    max_num_batched_tokens: 16384
    # Disable vLLM LoRA for Qwen3-VL multimodal stability; the trainer will
    # merge adapters and sync weights into vLLM on E-steps.
    enable_lora: false
    enable_prefix_caching: true
    sleep_level: 0

  # vLLM path currently supports greedy only.
  decode_mode: greedy
  # NOTE: Dense LVIS outputs can be very long; keep this large for real runs.
  # Ensure (prompt_len + max_new_tokens) <= vllm.max_model_len (and model max length).
  max_new_tokens: 8192
  decoding:
    temperature: 0.0
    top_p: 1.0
    top_k: -1
  # Mild anti-loop bias (empirically helpful on Qwen3-VL for long dense JSON).
  repetition_penalty: 1.05

  candidate_top_k: 5
  maskiou_gate: 0.3
  maskiou_resolution: 256
  fp_cost: 1
  fn_cost: 2

  ot_cost: l2
  ot_epsilon: 10
  ot_iters: 3


  # Optional desc monitoring (metrics only; does not affect loss).
  # Set mode=semantic (or both) to enable Sentence-Transformer similarity.
  desc_monitor:
    enabled: true
    mode: exact
    every_steps: 20
    max_pairs: 64
    semantic_model: sentence-transformers/all-MiniLM-L6-v2
    semantic_threshold: 0.6
    semantic_device: auto
    semantic_batch_size: 32
    semantic_max_length: 64
