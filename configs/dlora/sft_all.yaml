extends: ./sft_base.yaml

# Variant: apply DoRA/LoRA to all linear modules (no target_regex filter).
# Keeps SFT (no KD) and the same data/augmentation as sft_base.

model:
  model: model_cache/models/Qwen/Qwen3-VL-8B-Instruct  # Stage 2 checkpoint (base + LLM LoRA)

tuner:
  # Remove the narrow target_regex so all linear layers get DoRA adapters.
  target_regex: null
  lora_rank: 16
  lora_alpha: 32

training:
  learning_rate: 2.0e-4
  vit_lr: 1.0e-4
  aligner_lr: 6.0e-4
  run_name: epoch_30-dlora-lrs_2_1_6-sft_all
  warmup_ratio: 0.1
