extends: monitor_stage2_rollout_matching_ckpt3106_128_32_vllm.yaml

# Monitoring run with buffered rollouts (E-step generate once, M-step reuse).
#
# Notes:
# - `rollout_buffer.m_steps` counts optimizer steps (TrainerState.global_step units).
# - With m_steps>1, each gradient accumulation window is repeated m_steps times per rank.
# - Set dataloader_drop_last=true to avoid a final partial accumulation window (which cannot be safely repeated).

training:
  run_name: monitor_stage2_rollout_matching_ckpt3106_128_32_vllm_buffered
  output_dir: ./output/rollout_matching_sft/monitor_ckpt3106_buffered
  logging_dir: ./tb/rollout_matching_sft/monitor_ckpt3106_buffered

  # Prefer full windows only for reuse.
  dataloader_drop_last: true

custom:
  extra:
    rollout_matching:
      rollout_buffer:
        enabled: true
        # Reuse one rollout window across 4 optimizer steps (1x E-step + 3x M-steps).
        m_steps: 4

      # vLLM Qwen3-VL multimodal encoder can be sensitive to TP mode.
      # If you see vision shape mismatches, try running without TP (tensor_parallel_size: 1).
      vllm:
        tensor_parallel_size: 4
        # Keep this modest to leave headroom for the training model on the same GPU.
        gpu_memory_utilization: 0.50
        max_num_seqs: 4
        infer_batch_size: 4
        # Keep this small enough to fit KV cache in colocate mode when TP=1.
        max_model_len: 16000
        max_num_batched_tokens: 8192

      # With the reduced vLLM max_model_len above, keep generation shorter too.
      max_new_tokens: 8192

      # Optional: reduce peak memory during colocate vLLM rollouts (first-step engine init + LoRA sync + infer).
      # Not supported with DeepSpeed/ZeRO in this trainer.
      offload:
        enabled: false
        offload_model: true
        offload_optimizer: true
