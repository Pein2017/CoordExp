extends: sft_base.yaml

# Scheme A (Stage-1 retrain): CE on non-coord tokens + coord-gated softCE+W1 on coord tokens.
# This config is intentionally placeholder-heavy; tune per run.

global_max_length: 16000

model:
  # Base checkpoint (expanded vocab with <|coord_*|> tokens)
  # model: model_cache/Qwen3-VL-4B-Instruct-coordexp
  model: output/1-24/stage_1/poly_prefer_semantic_max60/epoch_4-softce_w1-LRs-2e-4_1e-4_4e-4-from-base-4B/checkpoint-1520_merged

training:
  output_dir: ./output/1-24/stage_1-continue-zero-area-removed/
  run_name: epoch_2-softCE-3-W1-1-gate-1-llm_lr-1e-4-vit_lr-0.5e-4-aligner_lr-2e-4
  logging_dir: ./tb/1-24/stage_1-continue-zero-area-removed/
  num_train_epochs: 2
  per_device_train_batch_size: 1
  per_device_eval_batch_size: 1
  effective_batch_size: 32
  # Use the custom optimizer variant so coord_offset can have its own lr buckets.
  optimizer: multimodal_coord_offset
  learning_rate: 1.0e-4
  vit_lr: 0.5e-4
  aligner_lr: 2.0e-4
  packing: true
  packing_buffer: 512
  packing_min_fill_ratio: 0.7
  packing_drop_last: true
  eval_packing: true
  eval_steps: 40
  save_steps: 200
  save_total_limit: 2
  save_delay_steps: 400

custom:
  train_jsonl: public_data/lvis/rescale_32_768_poly_prefer_semantic_max60/train.poly_prefer_semantic_cap20.max60.coord.jsonl
  val_jsonl: public_data/lvis/rescale_32_768_poly_prefer_semantic_max60/val.poly_prefer_semantic_cap20.max60.coord.jsonl
  val_sample_limit: 1000
  val_sample_with_replacement: false
  token_type_metrics:
    enabled: true
  # Train only the 1000 newly added coord-token IDs via a lightweight offset adapter
  # (keeps the adapter small vs saving full embed/lm_head matrices).
  coord_offset:
    enabled: true
    tie_head: true
    ids: { start: 151670, end: 152669 }  # <|coord_0|>.. <|coord_999|>
    # Optional: override per-bucket lrs; defaults to training.learning_rate when omitted.
    embed_lr: 1.0e-4
    head_lr: 1.0e-4
    weight_decay: 0.0
  coord_soft_ce_w1:
    enabled: true
    # Loss composition at coord positions: soft_ce_weight * softCE + w1_weight * W1(CDF)
    soft_ce_weight: 3
    w1_weight: 1.0
    gate_weight: 1.0
    # Distribution temperature for coord-gated softmax.
    temperature: 0.9
    # Gaussian soft target over bins 0..999.
    target_sigma: 1.0
    target_truncate: 8

debug:
  enabled: false
  output_dir: ./output_debug/
  train_sample_limit: 256
  val_sample_limit: 64
