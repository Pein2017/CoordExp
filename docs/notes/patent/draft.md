## 二、详细介绍技术背景

近年来，开放词汇目标检测（open-vocabulary object detection）与视觉-语言大模型（Vision-Language Large Model，简称 V-LLM）的结合逐渐成为研究热点。传统目标检测框架（如 Faster R-CNN、YOLO、DETR 及其变体）通常采用“视觉骨干网络 + 检测头”的架构，即：

1. 通过卷积神经网络（CNN）或视觉 Transformer（ViT）提取图像特征；
2. 在特征图上生成候选框或使用一组 query 进行解码；
3. 由检测头输出每个候选的类别预测与边界框回归结果。

其中，以 DETR 为代表的集合预测（set prediction）框架，引入了 Transformer 结构和匈牙利匹配（Hungarian matching）机制，将目标检测建模为一个**无序目标集合的匹配问题**。典型流程为：

* 使用视觉骨干网络提取图像特征；
* 使用固定数量的 query 通过 Transformer 解码器得到一组预测框 $\{\hat{b}_i\}$ 及其类别分布；
* 根据预测框与真实框之间的 L1 / GIoU 代价，通过匈牙利算法求解一一匹配；
* 在匹配结果基础上，对每个 query 对应的预测框施加回归损失和分类损失，对未匹配的预测施加“无目标（no-object）”损失。

这类方法在**集合训练、几何回归**方面已经形成较为成熟的技术体系，但其类别空间通常受限于预定义的类别集合，开放词汇能力有限。

为支持开放词汇检测，一些工作基于 CLIP 等图文对齐模型，在视觉检测器上增加文本分支：检测头输出每个候选框的视觉特征，文本编码器将类别名称或短语编码为文本特征，通过余弦相似度等方式进行匹配，从而支持任意文本类别。这类方法在类别开放性上取得进展，但整体架构仍然是“**视觉检测器 + 额外文本分支**”，未能充分利用大语言模型在复杂推理与自然语言生成方面的优势。

随着多模态大语言模型的发展，一些工作开始尝试直接使用 V-LLM 完成检测和 grounding 任务。其典型特征是：

* 输入图像和文本提示，由 V-LLM 生成包含目标描述与位置信息的文本序列；
* 位置信息通常以纯文本数字形式（例如“x1=123, y1=45, x2=456, y2=300”）嵌入在输出字符串中；
* 训练时多采用标准自回归 SFT（Supervised Fine-Tuning），即对完整输出序列施加 token 级交叉熵损失。

为减轻逐字符数字生成的难度，部分工作引入了“坐标离散 token”，将坐标值离散为若干区间 bin，并为每个 bin 分配特殊 token；也有工作在推理阶段对 V-LLM 的输出文本进行解析和后处理，再以 IoU 等指标评估。然而，这些几何指标多用于评估阶段，**没有形成从 V-LLM LM Head 到几何空间的端到端回归梯度路径**。

另一方面，针对 V-LLM 在检测任务上的表现，也有研究尝试采用强化学习（RL）或奖励建模（RM）方法，将检测指标（如 mAP、IoU、F1）转化为奖励信号，引导模型在自回归生成空间中优化输出质量。典型过程包括：

* 通过多次 rollout 采样不同的输出序列；
* 利用外部打分器或评估代码计算每条序列的检测质量得分；
* 使用 REINFORCE、PPO 等方法更新模型参数。

这类方案在理论上可以不依赖 token 级 GT 序列，直接对“集合级结果”施加强监督，但在实际工程中存在奖励稀疏、梯度方差大、训练不稳定、调参困难等问题，限制了在工业场景中的广泛应用。

在关键点检测、姿态估计等任务中，已有工作提出利用 heatmap 的 soft-argmax 或期望坐标来获得连续坐标估计：先对热力图做 softmax 得到各像素位置的概率分布，再以概率加权求和的方式计算关键点连续坐标，并在该坐标上施加 L2 / L1 等回归损失，形成可导的几何回归路径。这类方法验证了“**离散概率分布的期望可以作为连续输出的桥梁**”，但主要依赖 CNN-热力图等专用结构，尚未在**V-LLM 的 token 级输出**上，与集合预测和开放词汇检测进行统一设计。

综合来看，现有技术已经分别在以下方向有所突破：

* DETR 等框架提供了成熟的集合预测与匈牙利匹配机制；
* CLIP 等模型支持开放词汇类别表示；
* V-LLM 能生成包含检测信息的文本序列；
* soft-argmax 思想验证了从离散分布到连续数值的可导映射；
* 强化学习提供了基于任务指标的后验优化手段。

但目前仍缺乏一种在**统一的 V-LLM 架构中，同时实现开放词汇、集合预测、端到端几何回归、顺序无关训练**的完整方案，且要求在不引入额外检测头或复杂 RL 框架的前提下，仅通过词表设计与损失函数设计实现上述目标。

---

## 三、现有技术的缺点是什么？本发明的目的

### 3.1 现有技术的主要缺点

结合上述背景，可以归纳出现有 V-LLM 检测相关技术在以下几个方面存在明显不足：

1. **训练范式与任务本质不匹配**  
   现有基于 V-LLM 的检测方案大多采用序列级 SFT，将一组目标按照某种顺序线性化为文本序列，并对该序列执行 token 级交叉熵训练。然而，目标检测本质上是一个**无序目标集合**预测问题，而不是有序序列预测。当模型生成的目标集合与真实集合一致但顺序不同，即出现“对但顺序不对”的情况时，序列级交叉熵仍将其视为错误，导致：

   * 大量监督信号被浪费；
   * 梯度被强制拉向单一顺序；
   * 不利于模型学习到真正的“集合级”表示与推理。

2. **几何信息仅通过 token 级 CE 间接约束，梯度粗糙**  
   不论是纯文本数字形式还是离散坐标 token，现有 V-LLM 检测方案大多在训练时对这些坐标 token 使用交叉熵损失，即只关心“预测 token 是否等于 GT token”，没有考虑不同坐标 token 对应的物理位置接近与否，导致：

   * 某些错误坐标 token 与 GT 坐标非常接近，但损失与完全错误的 token 无差别；
   * 几何误差无法通过连续梯度逐步减小，只能通过“猜中 token”获得非零提升；
   * 与传统 DETR 中基于 L1 / GIoU 的连续回归相比，梯度信号非常粗糙，影响定位精度和稳定性。

3. **强化学习路线训练成本高、稳定性差**  
   使用 RL 将检测指标作为奖励反馈给 V-LLM 虽然理论上可行，但存在：

   * 需要多次 rollout，训练开销显著增加；
   * 奖励方差大，需引入复杂基线或值函数设计；
   * 容易出现训练不稳定甚至性能退化；
   * 工程实现与调参成本高，不利于在工业场景推广。

4. **soft-argmax 思想未与 V-LLM 的自回归输出深度融合**  
   虽然在关键点检测等领域已经证明利用概率期望进行可导的坐标回归是有效的，但现有工作主要在 CNN heatmap 结构中展开，并未解决以下问题：

   * 如何在 V-LLM 的 token 输出上定义类似的期望坐标机制；
   * 如何在不引入额外检测头的情况下，将 L1 / GIoU 直接作用于 LM Head；
   * 如何将该机制与集合匹配、开放词汇描述、顺序无关训练统一起来。

### 3.2 本发明的目的

针对上述问题，本发明的目的在于提供一种**基于坐标期望回归与集合匹配的 V-LLM 开放词汇目标检测方法及系统**，其总体目标包括：

1. **实现集合级、顺序无关的训练范式**  
   通过引入匈牙利匹配，将 V-LLM 输出的多目标结果解析为预测集合，与真实目标集合进行一一匹配，使得训练损失仅依赖集合匹配关系，而不依赖输出顺序，从而显式消除顺序敏感性。

2. **构建从坐标 token 概率分布到连续坐标的可导桥梁**  
   通过在词表中引入专门的坐标 token，并在坐标位置使用 softmax + 期望的方式，将 LM Head 输出的离散概率分布映射为连续坐标，使 L1 / GIoU 等几何回归损失可以直接作用于 LM Head，实现端到端的几何梯度传递。

3. **在统一 SFT 框架下融合文本生成与几何回归**  
   在保持 V-LLM 原有自回归训练机制的前提下，将 token 级交叉熵与基于期望坐标的几何损失统一到一个训练目标中，避免引入复杂的 RL 结构，使本方案具有良好的可实现性与工程稳定性。

4. **支持多样输出顺序与表达方式的后验感知训练（可选）**  
   在可选实施方式中，通过对同一输入进行多次 rollout，近似模型在不同输出顺序和语言表达下的后验分布，对每条 rollout 对应的损失进行平均，从而增强模型在不同排列和表述下的鲁棒性。

进一步地，相比现有 soft-argmax 与 DETR 等方案，本发明在技术路径上具有本质差异：

* 与基于 CNN heatmap 的 soft-argmax 不同，本发明的期望回归直接作用于 V-LLM LM Head 在**一维离散坐标 token 子集**上的概率分布，无需额外生成二维热力图或专用检测分支，天然兼容任意 Transformer 结构的多模态大模型；
* 与基于 DETR 的集合预测不同，DETR 通过专门的检测头将一组 query 映射为“类别 + 边界框”，而本发明直接在**自回归文本序列内部**编码对象描述与边界框，将集合匹配应用于由 LM Head 概率期望得到的对象集合，实现“语言生成—几何回归—集合预测”的统一；
* 与基于强化学习的后验优化不同，本发明始终在**完全可导的 SFT 框架**内工作，避免多次 rollout 和高方差 reward 估计，仅通过损失函数与词表设计实现集合级几何优化。

本发明所要解决的核心技术问题可以概括为：

> 在不引入额外检测头、不依赖复杂强化学习框架的前提下，如何在 V-LLM 的自回归架构内部，实现开放词汇目标检测所需的集合级顺序无关训练与端到端几何回归，同时保持训练过程的稳定性与工程实现的简洁性。

---

## 四、本发明技术方案的详细阐述（含实施例）

### 4.1 数学定义与符号约定

#### 输入与模型

输入图像 $x \in \mathbb{R}^{H \times W \times 3}$。

V-LLM 参数记为 $\theta$，其对图像和文本前缀给出自回归输出分布：

$$
p_\theta(y_{1:T} \mid x) = \prod_{t=1}^T p_\theta(y_t \mid x, y_{<t}),
$$

其中 $y_t$ 为第 $t$ 个输出 token，$T$ 为序列长度。

#### 真实目标集合与边界框表示

一张图像的真实目标集合记为：

$$
G = \{ g_j \}_{j=1}^M, \quad g_j = (d_j, b_j),
$$

其中：

- $M$ 为目标数量；
- $d_j$ 为第 $j$ 个目标的文本描述（文本序列）；
- $b_j$ 为边界框，形式为：

$$
b_j = \bigl(x^{(j)}_1, y^{(j)}_1, x^{(j)}_2, y^{(j)}_2\bigr), \quad x^{(j)}_1, y^{(j)}_1, x^{(j)}_2, y^{(j)}_2 \in [0,1],
$$

坐标已归一化到 $[0,1]$ 区间。

#### 词表划分与坐标 token 映射

将模型词表拆分为文本词表 $\mathcal{V}_{\text{text}}$ 与坐标词表 $\mathcal{V}_{\text{coord}}$，其中要求两者互不相交：

$$
\mathcal{V}_{\text{text}} \cap \mathcal{V}_{\text{coord}} = \varnothing.
$$

$$
\mathcal{V}_{\text{coord}} = \{ \langle \text{coord}_k \rangle \mid k = 0,1,\dots,K-1 \}.
$$

定义坐标 token 到归一化连续坐标的映射：

$$
\phi : \mathcal{V}_{\text{coord}} \to [0,1], \quad \phi(\langle \text{coord}_k \rangle) = \frac{k}{K-1}.
$$

真实边界框坐标通过线性量化映射到最近的坐标 bin，以构造 GT token 序列。可选地，坐标 token 仅用于编码边界框坐标，不在普通自然语言描述中出现，以避免语义与几何含义混用。

#### 对象子序列编码格式

对于第 $i$ 个对象，本发明采用固定格式的子序列编码，例如：

$$
[\langle \text{obj\_start} \rangle, d_i^{(1)}, \dots, d_i^{(L_i)}, c^{(i)}_{x1}, c^{(i)}_{y1}, c^{(i)}_{x2}, c^{(i)}_{y2}, \langle \text{obj\_end} \rangle],
$$

其中 $d_i^{(\ell)}$ 为描述 token，$c^{(i)}_{*}$ 为坐标 token。多个对象子序列串联构成完整输出序列。

一种实现方式是，$\langle \text{obj\_start} \rangle$ 与 $\langle \text{obj\_end} \rangle$ 用于标记对象子序列的边界，可选择性地纳入文本损失 $L_{\text{text}}$ 的计算；另一种实现方式是，仅对语义描述 token $d_i^{(\ell)}$ 计算交叉熵损失，而对结构标记不施加约束。本发明对上述等效变体均予以涵盖。

### 4.2 坐标期望回归（CoordExp）与几何损失

#### 坐标位置 logits 与概率分布

在某个坐标位置（例如 $x_1$）上，LM Head 输出 logits 向量 $z \in \mathbb{R}^{|\mathcal{V}|}$。在计算几何损失时，仅对坐标词表子集 $\mathcal{V}_{\text{coord}}$ 做 softmax：

$$
p_k = \frac{\exp(z_k / \tau)}{\sum_{k' \in \mathcal{V}_{\text{coord}}} \exp(z_{k'} / \tau)}, \quad k \in \mathcal{V}_{\text{coord}},
$$

其中 $\tau > 0$ 为温度参数。

#### 坐标期望解码

使用上式得到的坐标分布期望作为连续坐标估计：

$$
\hat{c} = \sum_{k \in \mathcal{V}_{\text{coord}}} \phi(k) \cdot p_k
$$

与现有技术中将坐标预测视为“选择唯一坐标 bin 并对其施加 one-hot 交叉熵”的方案不同，本发明不强制预测分布在某个 bin 上尖锐集中，而是允许在多个相邻 bin 上形成平滑分布，通过期望解码直接得到连续坐标并用几何损失进行约束。这样一方面减小了纯量化误差的上限，另一方面在真实坐标位于 bin 边界附近时，多个 bin 均可获得有效梯度，提升了优化过程的稳定性。

对四个坐标位置分别执行上述操作，可得到第 $i$ 个预测对象的连续边界框：

$$
\hat{b}_i = \left(\hat{x}^{(i)}_1, \hat{y}^{(i)}_1, \hat{x}^{(i)}_2, \hat{y}^{(i)}_2\right)
$$

在实际实现中，当期望解码得到的坐标不满足预设约束（例如 $\hat{x}^{(i)}_1 > \hat{x}^{(i)}_2$ 或超出 $[0,1]$ 区间）时，可通过坐标重排、裁剪或对宽高采用非负函数变换等方式进行纠正，以保证预测边界框的几何合法性。

从优化视角看，该期望解码机制与传统基于 one-hot 的离散分类有着本质不同。对于传统的坐标 token 交叉熵损失，任一错误 token 的惩罚强度仅与“是否等于 GT bin”相关，**与该 bin 所代表的坐标位置距离 GT 的远近几乎无关**，导致靠近 GT 的“次优坐标”与明显错误坐标在梯度上的待遇相同。

而在本发明中，几何损失 $L_{\text{coord}}$ 首先通过连续坐标 $\hat{c}$ 对真实坐标 $c$ 的偏差产生梯度，再由链式法则传递到各个 $p_k$ 和 logits $z_k$。直观上，距离 GT 坐标更近的 bin 在期望中贡献更大，其对应的 logits 在反向传播中会获得更“友好”的梯度更新方向。由此，本发明将原本“全或无”的离散监督替换为与坐标距离成连续关系的梯度场，使 LM Head 在坐标 token 维度上具备类似连续回归头的优化特性。

#### 几何损失（L1 + GIoU）

对于预测框 $\hat{b}_i$ 与匹配到的真实框 $b_j$，定义单对边界框的几何损失为：

$$
\begin{aligned}
\ell_{\text{coord}}(\hat{b}_i, b_j) &= \alpha \|\hat{b}_i - b_j\|_1 + \beta \left(1 - \mathrm{GIoU}(\hat{b}_i, b_j)\right)
\end{aligned}
$$

其中 $\alpha, \beta > 0$ 为权重。GIoU 可按现有技术公开的定义：

$$
\begin{aligned}
\mathrm{GIoU}(\hat{b}, b) &= \mathrm{IoU}(\hat{b}, b) - \frac{|C \setminus (\hat{b} \cup b)|}{|C|}
\end{aligned}
$$

$C$ 为同时包围 $\hat{b}$ 和 $b$ 的最小外接矩形。

由于 $\hat{b}_i$ 对 logits $z$ 的依赖通过 softmax 和线性期望建立，且 L1 / GIoU 对 $\hat{b}_i$ 可导，因此在匹配关系给定的前提下，上述几何损失对 LM Head logits 完全可导，实现了从坐标 token 概率分布到连续几何回归的端到端梯度路径。

### 4.3 集合匹配与顺序无关训练

#### 预测对象集合与真实集合

对一条输出序列进行解析，得到预测对象集合：

$$
\hat{G} = \{ \hat{g}_i \}_{i=1}^N, \quad \hat{g}_i = (\hat{d}_i, \hat{b}_i),
$$

其中 $N$ 为预测对象数量，$\hat{d}_i$ 为描述文本，$\hat{b}_i$ 为 CoordExp 得到的边界框。

#### 语义代价与几何代价

定义预测对象 $\hat{g}_i$ 与真实对象 $g_j$ 的综合代价：

$$
\begin{aligned}
C_{i,j} &= \lambda_{\mathrm{geo}} \cdot L_{\mathrm{coord}}(\hat{b}_i, b_j) + \lambda_{\mathrm{sem}} \cdot D_{\mathrm{sem}}(\hat{d}_i, d_j),
\end{aligned}
$$

其中 $\lambda_{\text{geo}}, \lambda_{\text{sem}} > 0$ 为权重，$D_{\text{sem}}$ 为语义距离，例如基于文本编码器嵌入的余弦距离：

$$
D_{\mathrm{sem}}(\hat{d}_i, d_j)
= 1 - \cos\bigl(\mathbf{e}(\hat{d}_i), \mathbf{e}(d_j)\bigr).
$$

一种实现方式是，嵌入函数 $\mathbf{e}(\cdot)$ 由与 V-LLM 共享参数的文本编码模块实现；另一种实现方式是，$\mathbf{e}(\cdot)$ 由独立预训练并冻结的句向量模型实现。上述语义距离项在匹配阶段仅用于构造代价矩阵 $C_{i,j}$，不必对其输入的离散 token 求梯度，从而避免了由离散采样带来的不可导问题。

#### 匈牙利匹配

利用匈牙利算法在 $\hat{G}$ 与 $G$ 之间构造一一匹配：

$$
\sigma^*
= \arg\min_{\sigma \in \Pi}
\sum_{i=1}^{\min(N, M)} C_{i, \sigma(i)},
$$

其中 $\Pi$ 为所有匹配关系的集合。匹配成功的对 $(\hat{g}_i, g_{\sigma^*(i)})$ 作为正样本参与损失计算，未匹配的预测可视为多余预测或幻觉，可根据不同实现方式选择施加惩罚或忽略。

匈牙利匹配作为离散组合优化步骤，仅在前向阶段用于确定预测集合与真实集合之间的对应关系，不参与梯度回传；损失对模型参数的梯度仅通过已确定匹配对内部的几何损失等可导项进行反向传播。

在几何损失部分，由于匹配关系是基于集合级代价矩阵自动选取的，与对象在输出序列中的排列无关，因此几何监督在对象集合层面具有顺序不变性；文本自回归损失仍可采用预设的对象排序规则（如按 $x_1$ 从小到大），或结合 4.5 节所述的多顺序 Monte Carlo 近似进一步减弱排序偏差。

在一种基础实现方式中，训练阶段始终采用 teacher forcing 机制，即输入 token 序列完全由标注数据确定，对象子序列的顺序由预设规则（如按 $x_1$ 升序）固定。此时，匈牙利匹配主要用于在**预测边界框集合 $\{\hat{b}_i\}$ 与真实边界框集合 $\{b_j\}$ 之间重建最优配对关系**，使几何损失在集合层面不依赖对象在序列中的排列，从而实现“几何层面的顺序无关训练”，而无需改变标准 SFT 的前向流程。

### 4.4 统一损失函数与训练流程

#### token 级文本损失

根据匹配关系，为每个真实对象构造相应的 GT token 子序列，并串联为整体 GT 序列 $y^{1:T}$。采用标准自回归交叉熵损失：

$$
L_{\text{text}}
= - \sum_{t=1}^T \log p_\theta\bigl(y^t \mid x, y^{<t}\bigr).
$$

一种实现方式是，$L_{\text{text}}$ 主要作用于描述 token 及对象边界标记等文本位置，对坐标 token 所在位置不施加交叉熵损失，而仅通过几何损失 $L_{\text{coord}}$ 进行约束；另一种实现方式是，在坐标 token 所在位置对离散 bin index 施加辅助的分类损失，与几何回归损失共同优化坐标预测质量。本发明对上述等效变体均予以保护。

#### 集合级几何损失

在 $\sigma^*$ 基础上对所有匹配对累积几何损失：

$$
L_{\text{coord}}
= \sum_{i=1}^{\min(N,M)} \ell_{\text{coord}}\bigl(\hat{b}_i, b_{\sigma^*(i)}\bigr).
$$

#### 总体损失

本发明采用如下统一训练目标：

$$
L_{\text{total}}
= L_{\text{text}} + \lambda_{\text{coord}} \cdot L_{\text{coord}},
$$

其中 $\lambda_{\text{coord}} > 0$ 为几何损失权重。该目标完全兼容现有 SFT 流程，只需在坐标位置增加 CoordExp 与几何损失的计算。

### 4.5 多种对象排列的 Monte Carlo 训练（可选实现方式）

本发明可以对同一输入图像的真实目标集合 $G$ 构造多种不同的对象排列，以减弱固定排序对训练的影响。具体地，针对每个样本随机采样 $K$ 种排列 $\{\pi_k\}_{k=1}^K$，并据此构造 $K$ 条 GT token 序列 $\{y^{(k)}\}_{k=1}^K$。

对于每条 GT 序列 $y^{(k)}$，在 teacher forcing 机制下进行前向传播，得到对应的文本损失与几何损失：

$$
L_{\text{total}}^{(k)}
= L_{\text{text}}^{(k)} + \lambda_{\text{coord}} \cdot L_{\text{coord}}^{(k)}.
$$

然后对 $K$ 条序列的损失做均值：

$$
L_{\text{MC}}
= \frac{1}{K} \sum_{k=1}^K L_{\text{total}}^{(k)}.
$$

该形式可视为对“对象顺序后验分布”下期望损失的 Monte Carlo 近似，使模型在多种合理排列下均能获得稳定训练信号，从而减弱特定排序选择带来的偏差。由于整个过程始终采用 teacher forcing，$L_{\text{MC}}$ 对模型参数的梯度可通过标准链式法则直接计算，无需显式引入基于 reward–logProb 的强化学习方法。

### 4.6 实施例

#### 实施例一：基于 teacher forcing 的 CoordExp 集合训练（主实现方式）

1. 选取任意预训练 V-LLM（如 Qwen-VL 类模型）作为基础，通过扩展词表加入坐标 token 集合 $\mathcal{V}_{\text{coord}}$。

2. 对每张图像及其真实目标集合 $G$，按照某种固定规则（如按 $x_1$ 从小到大排序）排列对象，构造 GT token 序列。

3. 使用 teacher forcing 对 GT 序列前向传播，得到所有 token 位置的 logits，定位出坐标位置的 logits。

4. 在坐标位置执行 CoordExp 期望解码，得到预测边界框集合 $\hat{G} = \{\hat{g}_i\}_{i=1}^N$。一种基础实现方式是，将第 $i$ 个预测对象 $\hat{g}_i$ 与序列中第 $i$ 个真实对象 $g_i$ 对应，直接计算几何损失；另一种实现方式是，按 4.3 节所述引入匈牙利匹配，以集合形式重建预测集合与真实集合之间的对应关系，进一步减弱固定排序对几何监督的影响。

5. 按上述公式计算文本损失 $L_{\text{text}}$ 与几何损失 $L_{\text{coord}}$，构造总体损失 $L_{\text{total}}$，进行反向传播与参数更新。整个过程完全遵循标准的自回归 teacher forcing 训练范式，仅在损失计算阶段引入 CoordExp 与集合匹配，因此训练稳定、实现成本低。

#### 实施例二：引入自由生成与集合匹配的一致性训练（可选增强方式）

在另一种可选实现方式中，可针对已经通过实施例一预训练的模型，引入部分自由生成（rollout）阶段，以进一步消除对象顺序与结构化编码方式对训练的影响。

1. 对于给定输入图像及其真实目标集合 $G$，首先使用 V-LLM 在给定文本提示下执行自由生成，得到一条或多条完整输出序列 $\tilde{y}_{1:T}$，并据此解析出模型自主生成的预测对象集合 $\tilde{G} = \{\tilde{g}_i\}_{i=1}^{\tilde{N}}$。

2. 基于 4.3 节所述的代价矩阵设计，利用匈牙利算法在 $\tilde{G}$ 与 $G$ 之间构造集合级匹配关系，将几何损失 $L_{\text{coord}}$ 直接施加于“模型实际生成的对象排列及内容”所对应的边界框对。

3. 可选地，对匹配到的描述文本对 $(\tilde{d}_i, d_j)$ 施加额外的一致性约束（例如对比损失或 KL 距离），以鼓励模型在自由生成时同时保持语义描述与几何位置的一致性。

4. 前述自由生成阶段可以与 teacher forcing 阶段交替进行，或仅在训练后期以较低频率插入，用于改善模型在真实推理模式下的集合级行为。由于损失仍通过 CoordExp 与集合匹配构建，整体训练过程保持可导，无需显式引入基于 reward–logProb 的强化学习算法。

通过该可选实施例，本发明不仅在几何层面实现了顺序无关训练，还在“对象级输出结构”层面引入了集合一致性约束，使模型在面对不同排列、不同语言表述乃至自由生成模式时，均能保持较为稳定的检测与定位性能。

在不背离本发明整体构思的前提下，上述实施例中对象序列编码格式、坐标量化颗粒度 $K$、语义距离度量方式 $D_{\text{sem}}$、损失权重 $\lambda_{\text{coord}}, \lambda_{\text{geo}}, \lambda_{\text{sem}}$ 等均可进行等效替换和调整。

---

## 五、本发明/实用新型的关键点和保护点

本发明的核心创新点如下，将作为后续权利要求的主要保护对象：

（1）基于坐标期望回归的连续几何回归机制（核心）  
本发明在大模型词表中引入坐标 token，并以 softmax 概率分布的期望值作为连续坐标估计，使 L1 和 GIoU 等几何损失能够直接作用于语言模型的输出层，实现端到端的几何可导回归路径。该机制不依赖检测头，解决了传统 token 级交叉熵无法充分表达几何误差的问题。

（2）面向 V-LLM 序列输出的集合级匈牙利匹配方法（核心）  
本发明将模型输出解析为预测对象集合，并结合语义距离与几何距离构造代价矩阵，通过匈牙利算法完成预测集合与真实集合的顺序无关匹配。该设计首次在 V-LLM 中实现 DETR 式集合训练，使监督信号摆脱输出顺序限制，大幅提升有效监督率。

（3）统一文本生成与几何回归的端到端损失函数（核心）  
本发明构建统一的训练目标，将文本交叉熵损失与几何回归损失进行联合优化，实现大模型自然语言生成能力与边界框定位能力的协同训练。该机制无需强化学习或额外网络结构，训练稳定，工程可行性高。

（4）多对象排序的 Monte Carlo 后验感知训练机制（可选增强）  
本发明提出多序列教师强制的 Monte Carlo 方案，通过对真实目标集合随机采样多种排列并对多条 GT 序列计算损失取均值，近似对象排序后验的期望风险，进一步降低排序偏差并提升泛化性能。整个过程保持全可导性，不依赖高方差的强化学习方法。

（5）无需额外检测头的开放词汇检测系统架构（核心组合）  
本发明使用统一的 V-LLM 架构即可完成开放词汇描述、边界框预测、集合匹配和端到端训练，无需构建独立检测头或额外视觉回归网络，系统结构简洁、参数复用高，有利于多任务集成与部署。

综上，本发明的关键保护点包括：以坐标期望回归机制、集合匹配驱动的顺序无关训练方法、统一文本与几何的端到端损失设计以及无需检测头的整体系统架构构成的**核心技术组合**，以及在此基础上引入的多顺序 Monte Carlo 后验感知训练等可选增强模块。上述技术点共同构成本发明的核心原创部分，其中核心技术组合可作为独立权利要求的基础，可选增强模块可通过从属权利要求加以进一步限定与保护。

---

## 六、针对第三部分的技术方案，是否有其他替代方案同样能完成本发明/实用新型目的？

**有☑  无□**

尽管本发明提出的技术方案在一致性、可导性与工程可行性方面具有较优综合表现，但从理论上看，仍然存在若干在技术效果上与本发明基本一致的替代实现路径。为了明确等效保护范围，现对典型替代方案进行说明（在不改变“集合匹配 + 连续可导坐标回归 + 统一损失”这一技术效果前提下）：

（1）替代的坐标回归方式  
可将坐标 token 的期望回归替换为其他连续可导的映射方式，例如采用基于 sigmoid 的连续坐标解码、tanh 映射、或利用混合分布（如多峰高斯）的加权均值等策略。只要仍然通过 LM Head 在坐标 token 子空间上的连续可导映射获得边界框坐标，并在该坐标上施加 L1/GIoU 等几何损失，其技术效果与本发明的 CoordExp 机制基本一致，可视为等效设计。

（2）替代的集合匹配机制  
除匈牙利匹配外，还可引入最优传输（optimal transport）方法、Sinkhorn-Knopp 软匹配、可微排序（如 NeuralSort）等技术，构建预测集合与真实集合的对应关系。只要在 V-LLM 的自回归输出基础上实现“集合级顺序无关监督”，并将几何损失建立在该对应关系之上，都可视为本发明集合匹配模块的等效实现。

（3）替代的对象结构化编码方式  
对象的描述序列可以采用 XML/JSON 结构化编码、图结构标记、分层 token 编码等形式，只要能够明确解析为对象集合与其边界框，都属于本发明的技术范畴。对象边界符、字段标记、位置编码等均可以互换。

（4）替代的后验近似训练策略  
除了 Monte Carlo 多序列教师强制外，也可引入：带噪声扰动的软排序数据增强、基于对比学习的多顺序一致性约束、或利用可微近似匹配函数实现排序无关训练。核心目标是一致性学习，而非具体实现方式。

（5）替代的文本—视觉监督方式  
可选地，可将语义距离替换为预训练视觉语言模型的嵌入相似度，或通过视觉特征直接参与匹配计算；也可添加轻量的文本—视觉对比学习模块。只要最终损失仍由“集合匹配 + 几何回归”驱动，其技术效果保持一致，属于等效实现。

综上，凡基于 V-LLM 自回归框架，在不引入独立检测头和复杂强化学习结构的前提下，以等效方式同时实现：（1）顺序无关的集合匹配监督；（2）连续可导的边界框坐标回归；（3）统一文本和几何的端到端训练目标，其整体技术效果与本发明基本一致，应视为本发明的等效替代方案。

---

## 七、请说明本发明/实用新型技术方案的落地可能性？

填写说明：请分析技术方案的落地可能性并附相关说明。

如：已确定或计划应用于XX版本的XX产品中；或者已确定或计划纳入XX标准中。

（1）可直接集成到现有 V-LLM 框架，无需修改核心架构  

本发明基于词表扩展、损失函数设计和集合匹配机制实现端到端训练，不需对 Transformer 主体结构做任何调整，能够直接复用现有的 Qwen-VL、LLaVA、InternVL 等主流 V-LLM 模型，因此落地成本极低。

（2）已具备明确的实验验证路径，预计在下一版本模型中部署测试  

本方案可快速在 COCO、LVIS 等公开数据集上构建实验验证。由于训练流程与传统 SFT 完全兼容，可在现有训练脚本中仅修改损失模块即可运行，预计可纳入某些 V-LLM 的下一个实验版本，用于开放词汇检测能力的增强测试。

在典型设置下，本发明方案在与仅使用 token 级交叉熵的 V-LLM 检测基线相比，有望在 COCO 等基准数据集上获得 mAP@0.5–0.95 的实质性提升，同时训练过程中损失曲线更加平滑、收敛速度更快。上述预期效果来源于 CoordExp 提供的连续几何梯度与集合匹配带来的高效监督利用率。

（3）可在 AI 质检、垂直领域检测等应用中直接落地  

本发明已在多模态质检项目中规划试验，将用于提升模型对机房环境、设备异常、布线错误等对象的定位精度。依托统一 V-LLM 架构，可在不增加额外检测头的情况下快速上线，适用于大规模工程系统。

在机房质检等细粒度场景中，本发明尤其有助于提升螺丝、挡板、标签等小目标的召回率与定位精度，并减少因 RL 训练不稳定带来的工程风险，为大规模质检系统提供更可靠的基础能力模块。

（4）具备产业化推广潜力，可纳入未来 V-LLM 能力标准  

随着开放词汇检测成为行业趋势，本发明提出的“坐标期望回归 + 集合匹配 + 顺序无关训练”机制有潜力成为未来多模态大模型中通用的检测能力模块，在多个机构主导的 V-LLM 能力对齐基准中具备纳入标准的可能性。

（5）可快速适配边缘部署、云端推理等多类型产品形态  

由于推理阶段不涉及额外计算，仅需生成文本序列并解析坐标 token，计算成本与原模型相同，可方便集成至云端 API、企业内部质检平台、移动设备轻量化模型等多种场景。

综上所述，本发明技术方案的落地条件成熟，预计可在短期内在实验系统与质检系统中率先验证，并具备进一步纳入更大规模 V-LLM 产品体系的可行性。
